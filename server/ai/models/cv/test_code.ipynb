{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 112, 92, 64)       640       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 92, 64)       36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 46, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 46, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 46, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 23, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 23, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 39424)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               20185600  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 40)                20520     \n",
      "=================================================================\n",
      "Total params: 21,350,376\n",
      "Trainable params: 21,350,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "original_model = load_model('./data/0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL/original_model.h5py')\n",
    "original_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import keras.backend as K\n",
    "\n",
    "# dirname = os.path.dirname(__file__)\n",
    "# print(dirname)\n",
    "# root_path = os.path.join(dirname, 'data')\n",
    "\n",
    "# call\n",
    "def predict_fomular_model(images, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    '''\n",
    "    predict for images with specified model\n",
    "    '''\n",
    "    images = preprocess_image(images)\n",
    "\n",
    "    # find the size M, N\n",
    "    M = images.shape[1]\n",
    "\n",
    "    predict_model, (M_s, N_s) = load_model_by_size(M, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id)\n",
    "\n",
    "    # interpolate image by size\n",
    "    print(images.shape)\n",
    "    rs_images = []\n",
    "    for i in range(images.shape[0]):\n",
    "        rs_images.append(resize(images[i], (M_s, N_s)))\n",
    "    \n",
    "    rs_images = np.asarray(rs_images, dtype='float64')\n",
    "    preds = predict_model.predict(rs_images, verbose=0)\n",
    "    \n",
    "    return np.argmin(preds, axis=1), np.max(preds, axis=1)\n",
    "\n",
    "# call\n",
    "def predict_original_model(images, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    root_path = './data'\n",
    "    '''\n",
    "    predict for images with specified model\n",
    "    --------------------------------------------\n",
    "    return\n",
    "        labels, accuracies\n",
    "    '''\n",
    "    K.clear_session()\n",
    "    # preprocess r, alpha, beta, gamma, delta, e_s, e_b\n",
    "    r_str = \"{0:.4f}\".format(round(r,4))\n",
    "    alpha_str =  \"{0:.4f}\".format(round(alpha,4))\n",
    "    beta_str =  \"{0:.4f}\".format(round(beta,4))\n",
    "    gamma_str =  \"{0:.4f}\".format(round(gamma,4))\n",
    "    delta_str =  \"{0:.4f}\".format(round(delta,4))\n",
    "    e_s_str =  \"{0:.4f}\".format(round(e_s,4))\n",
    "    e_b_str =  \"{0:.4f}\".format(round(e_b,4))\n",
    "    \n",
    "    fomulars = os.listdir(root_path)\n",
    "\n",
    "    direct = r_str +'_' + alpha_str +'_' + beta_str +'_' + gamma_str +'_' + delta_str +'_' + e_s_str +'_' + e_b_str + '_' + dataset_id\n",
    "    if direct in fomulars:\n",
    "        model_path = os.path.join(root_path, direct, 'models', 'original_model.h5py')\n",
    "\n",
    "    images = preprocess_image(images)\n",
    "    \n",
    "    original_model = load_model(model_path)\n",
    "    print(original_model.inputs)\n",
    "\n",
    "    interpolated_imgs = []\n",
    "    for i in range(images.shape[0]):\n",
    "        i_images = interpolate_image(images[i], (int(original_model.inputs[0].shape[1]), int(original_model.inputs[0].shape[2])))\n",
    "        interpolated_imgs.append(i_images)\n",
    "    \n",
    "    interpolated_imgs = np.asarray(interpolated_imgs)\n",
    "    interpolated_imgs = np.concatenate(interpolated_imgs, axis=0)\n",
    "    \n",
    "    preds = original_model.predict(interpolated_imgs, verbose=0)\n",
    "    return np.argmax(preds, axis=1), np.max(preds, axis=1), interpolated_imgs\n",
    "\n",
    "\n",
    "def preprocess_image(images):\n",
    "    images = np.asarray(images, dtype='float64')\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "       \n",
    "    for image in images:\n",
    "        if np.max(image) > 1:\n",
    "            image /= 255\n",
    "    return images\n",
    "\n",
    "def load_model_by_size(M, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    '''\n",
    "    load model and history by size M\n",
    "\n",
    "    return model, size\n",
    "    '''\n",
    "    # preprocess r, alpha, beta, gamma, delta, e_s, e_b\n",
    "    r_str = \"{0:.4f}\".format(round(r,4))\n",
    "    alpha_str =  \"{0:.4f}\".format(round(alpha,4))\n",
    "    beta_str =  \"{0:.4f}\".format(round(beta,4))\n",
    "    gamma_str =  \"{0:.4f}\".format(round(gamma,4))\n",
    "    delta_str =  \"{0:.4f}\".format(round(delta,4))\n",
    "    e_s_str =  \"{0:.4f}\".format(round(e_s,4))\n",
    "    e_b_str =  \"{0:.4f}\".format(round(e_b,4))\n",
    "    \n",
    "    fomulars = os.listdir('./data')\n",
    "\n",
    "    direct = r_str +'_' + alpha_str +'_' + beta_str +'_' + gamma_str +'_' + delta_str +'_' + e_s_str +'_' + e_b_str + '_' + str(dataset_id)\n",
    "\n",
    "    if direct in fomulars:\n",
    "        print(direct)\n",
    "        # load model by size\n",
    "        model_paths = os.listdir(os.path.join('./data',direct, 'models'))\n",
    "        history_paths = os.listdir(os.path.join('./data',direct, 'histories'))\n",
    "        \n",
    "        M_sizes = []\n",
    "        N_sizes = []\n",
    "        \n",
    "        for k in model_paths:\n",
    "            if 'original' in k:\n",
    "                continue\n",
    "            \n",
    "            M_sizes.append(int(k.split('_')[0]))\n",
    "            N_sizes.append(int(k.split('_')[1]))\n",
    "\n",
    "        min_index = find_cloest_size(M, M_sizes)\n",
    "\n",
    "        model_path = os.path.join('./data',direct, 'models', model_paths[min_index])\n",
    "        history_path = os.path.join('./data', direct, 'histories', history_paths[min_index])\n",
    "\n",
    "        print(model_path)\n",
    "        print(history_path)\n",
    "\n",
    "        with open(history_path, 'rb') as dt:\n",
    "            history = pickle.load(dt)\n",
    "\n",
    "        history['epochs'] = len(history['val_acc'])\n",
    "\n",
    "        return load_model(model_path), history, (M_sizes[min_index], N_sizes[min_index])\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_cloest_size(M, sizes):\n",
    "    '''\n",
    "    find cloest sizes for M\n",
    "    '''\n",
    "\n",
    "    dist = np.array([abs(M-k) for k in sizes])\n",
    "    return np.argmin(dist)\n",
    "\n",
    "def interpolate_image(image, original_size):\n",
    "    rs = []\n",
    "    # nb, bilinear, bicubic\n",
    "\n",
    "    orders = [0,1,3]\n",
    "    for i in orders:\n",
    "        t_image = resize(image, (original_size), order=i)\n",
    "        rs.append(t_image)\n",
    "\n",
    "    return rs\n",
    "\n",
    "# model, history, (_,_) = load_model_by_size(11, 0.2, 2.5, 0.5, 1.0, 0.5, 0, 0, 'ORL')\n",
    "# print(history)\n",
    "\n",
    "# model_path = './data/0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL/original_model.h5py'\n",
    "# img_path = './temp_0_2_0.png'\n",
    "\n",
    "# from skimage.io import imread\n",
    "\n",
    "# image = imread(img_path)\n",
    "# image = preprocess_image(image)\n",
    "\n",
    "# preds, probs = predict_original_model(image, model_path)\n",
    "# print(preds.shape)\n",
    "# print(preds)\n",
    "\n",
    "# print(probs.shape)\n",
    "# print(probs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "import tensorflow\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# call\n",
    "def predict_fomular_model(images, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    '''\n",
    "    predict for images with specified model\n",
    "    '''\n",
    "    images = preprocess_image(images)\n",
    "\n",
    "    # find the size M, N\n",
    "    M = images.shape[1]\n",
    "\n",
    "    predict_model, (M_s, N_s) = load_model_by_size(M, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id)\n",
    "\n",
    "    # interpolate image by size\n",
    "    print(images.shape)\n",
    "    rs_images = []\n",
    "    for i in range(images.shape[0]):\n",
    "        rs_images.append(resize(images[i], (M_s, N_s)))\n",
    "    \n",
    "    rs_images = np.asarray(rs_images, dtype='float64')\n",
    "    preds = predict_model.predict(rs_images, verbose=0)\n",
    "    \n",
    "    return np.argmin(preds, axis=1), np.max(preds, axis=1)\n",
    "\n",
    "# call\n",
    "def predict_original_model(images, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    # preprocess r, alpha, beta, gamma, delta, e_s, e_b\n",
    "    r_str = \"{0:.4f}\".format(round(r,4))\n",
    "    alpha_str =  \"{0:.4f}\".format(round(alpha,4))\n",
    "    beta_str =  \"{0:.4f}\".format(round(beta,4))\n",
    "    gamma_str =  \"{0:.4f}\".format(round(gamma,4))\n",
    "    delta_str =  \"{0:.4f}\".format(round(delta,4))\n",
    "    e_s_str =  \"{0:.4f}\".format(round(e_s,4))\n",
    "    e_b_str =  \"{0:.4f}\".format(round(e_b,4))\n",
    "    \n",
    "    fomulars = os.listdir('./data')\n",
    "\n",
    "    direct = r_str +'_' + alpha_str +'_' + beta_str +'_' + gamma_str +'_' + delta_str +'_' + e_s_str +'_' + e_b_str + '_' + str(dataset_id)\n",
    "    if direct in fomulars:\n",
    "        model_path = os.listdir(os.path.join('./data',direct, 'models', 'original_model.h5py'))\n",
    "\n",
    "    images = preprocess_image(images)\n",
    "    \n",
    "    original_model = load_model(model_path)\n",
    "    print(original_model.inputs)\n",
    "\n",
    "    interpolated_imgs = []\n",
    "    for i in range(images.shape[0]):\n",
    "        i_images = interpolate_image(images[i], (int(original_model.inputs[0].shape[1]), int(original_model.inputs[0].shape[2])))\n",
    "        interpolated_imgs.append(i_images)\n",
    "    \n",
    "    interpolated_imgs = np.asarray(interpolated_imgs)\n",
    "    interpolated_imgs = np.concatenate(interpolated_imgs, axis=0)\n",
    "    \n",
    "    preds = original_model.predict(interpolated_imgs, verbose=0)\n",
    "    return np.argmax(preds, axis=1), np.max(preds, axis=1)\n",
    "\n",
    "\n",
    "def preprocess_image(images):\n",
    "    images = np.asarray(images, dtype='float64')\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "       \n",
    "    for image in images:\n",
    "        if np.max(image) > 1:\n",
    "            image /= 255\n",
    "    return images\n",
    "\n",
    "def load_model_by_size(M, r, alpha, beta, gamma, delta, e_s, e_b, dataset_id):\n",
    "    '''\n",
    "    load model and history by size M\n",
    "\n",
    "    return model, size\n",
    "    '''\n",
    "    # preprocess r, alpha, beta, gamma, delta, e_s, e_b\n",
    "    r_str = \"{0:.4f}\".format(round(r,4))\n",
    "    alpha_str =  \"{0:.4f}\".format(round(alpha,4))\n",
    "    beta_str =  \"{0:.4f}\".format(round(beta,4))\n",
    "    gamma_str =  \"{0:.4f}\".format(round(gamma,4))\n",
    "    delta_str =  \"{0:.4f}\".format(round(delta,4))\n",
    "    e_s_str =  \"{0:.4f}\".format(round(e_s,4))\n",
    "    e_b_str =  \"{0:.4f}\".format(round(e_b,4))\n",
    "    \n",
    "    fomulars = os.listdir('./data')\n",
    "\n",
    "    direct = r_str +'_' + alpha_str +'_' + beta_str +'_' + gamma_str +'_' + delta_str +'_' + e_s_str +'_' + e_b_str + '_' + str(dataset_id)\n",
    "\n",
    "    if direct in fomulars:\n",
    "        print(direct)\n",
    "        # load model by size\n",
    "        model_paths = os.listdir(os.path.join('./data',direct, 'models'))\n",
    "        history_paths = os.listdir(os.path.join('./data',direct, 'histories'))\n",
    "        \n",
    "        M_sizes = []\n",
    "        N_sizes = []\n",
    "        \n",
    "        for k in model_paths:\n",
    "            if 'original' in k:\n",
    "                continue\n",
    "            \n",
    "            M_sizes.append(int(k.split('_')[0]))\n",
    "            N_sizes.append(int(k.split('_')[1]))\n",
    "\n",
    "        min_index = find_cloest_size(M, M_sizes)\n",
    "\n",
    "        model_path = os.path.join('./data',direct, 'models', model_paths[min_index])\n",
    "        history_path = os.path.join('./data', direct, 'histories', history_paths[min_index])\n",
    "\n",
    "        print(model_path)\n",
    "        print(history_path)\n",
    "\n",
    "        with open(history_path, 'rb') as dt:\n",
    "            history = pickle.load(dt)\n",
    "\n",
    "        history['epochs'] = len(history['val_acc'])\n",
    "\n",
    "        return load_model(model_path), history, (M_sizes[min_index], N_sizes[min_index])\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def find_cloest_size(M, sizes):\n",
    "    '''\n",
    "    find cloest sizes for M\n",
    "    '''\n",
    "\n",
    "    dist = np.array([abs(M-k) for k in sizes])\n",
    "    return np.argmin(dist)\n",
    "\n",
    "def interpolate_image(image, original_size):\n",
    "    rs = []\n",
    "    # nb, bilinear, bicubic\n",
    "\n",
    "    orders = [0,1,3]\n",
    "    for i in orders:\n",
    "        t_image = resize(image, (original_size), order=i)\n",
    "        rs.append(t_image)\n",
    "\n",
    "    return rs\n",
    "\n",
    "# model, history, (_,_) = load_model_by_size(11, 0.2, 2.5, 0.5, 1.0, 0.5, 0, 0, 'ORL')\n",
    "# print(history)\n",
    "\n",
    "# model_path = './data/0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL/original_model.h5py'\n",
    "# img_path = './temp_0_2_0.png'\n",
    "\n",
    "# from skimage.io import imread\n",
    "\n",
    "# image = imread(img_path)\n",
    "# image = preprocess_image(image)\n",
    "\n",
    "# preds, probs = predict_original_model(image, model_path)\n",
    "# print(preds.shape)\n",
    "# print(preds)\n",
    "\n",
    "# print(probs.shape)\n",
    "# print(probs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-85e22e1e6cb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "del(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'conv2d_1_input:0' shape=(?, 112, 92, 1) dtype=float32>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([17, 17, 17, 24, 24, 24], dtype=int64),\n",
       " array([0.9999993 , 0.9999999 , 1.        , 0.5858703 , 0.7098281 ,\n",
       "        0.56982934], dtype=float32),\n",
       " (6, 112, 92, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_path = './data/0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL/original_model.h5py'\n",
    "img_path = ['./21_7.png', './56_18.png']\n",
    "\n",
    "from skimage.io import imread\n",
    "\n",
    "images = [imread(k) for k in img_path]\n",
    "\n",
    "# images = preprocess_image(images)\n",
    "preds, labels, i_images = predict_original_model(images, 0.2,2.5,0.5,1.,0.5,0.,0.,'ORL')\n",
    "preds, labels, i_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "from skimage.io import imsave\n",
    "\n",
    "imsave('test.png', i_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './data/0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL/original_model.h5py'\n",
    "img_path = ['./21_7.png', './56_18.png']\n",
    "\n",
    "from skimage.io import imread\n",
    "\n",
    "images = [imread(k) for k in img_path]\n",
    "\n",
    "# images = preprocess_image(images)\n",
    "preds, labels = predict_original_model(images, model_path)\n",
    "preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2000_2.5000_0.5000_1.0000_0.5000_0.0000_0.0000_ORL\n",
      "(2, 22, 18, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([9, 6], dtype=int64), array([0.82885 , 0.991886], dtype=float32))"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, labels = predict_fomular_model(images, 0.2, 2.5, 0.5, 1.0, 0.5, 0.0, 0.0, 'ORL')\n",
    "preds, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.       , 0.9999881], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 18, 1)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.random.rand(26,21,1)\n",
    "t = resize(t, (22,18))\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9848904 , 0.98271817, 0.9926524 ], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-72612b71c999>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2503\u001b[0m     \"\"\"\n\u001b[0;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[1;32m-> 2505\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "np.max(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
